# --- External Strategy References ---
# This section lists external files containing specific strategies or instructions.
# The LLM assistant should read and incorporate these strategies as needed.
external_strategies:
  prompt_caching:
    file_path: "context_portal/prompt_caching_strategy.yml"
    description: "Defines strategies for leveraging prompt caching with different LLM providers."

cascade_conport_integration:
  introduction: |
    These instructions guide Windsurf Cascade on integrating with the ConPort project memory system (MCP plugin).
    Treat ConPort as a primary source for "MEMORIES" to guide your work.

  purpose: |
    ConPort stores and retrieves structured information about the current workspace, including:
    - Overall project goals and architecture (Product Context).
    - Current work focus, recent changes, and open questions (Active Context).
    - Key decisions, rationale, and implementation details.
    - Task progress and status.
    - Reusable system patterns.
    - Project-specific glossary terms and other custom data.

  workspace_id:
    source_description: |
      ConPort tools require a `workspace_id`. Derive this from the workspace URI provided in your context (e.g., parse 'file:///path/to/project' to '/path/to/project').
    fallback: |
      If the URI is not a local file path or parsing fails, you may need to ask the USER for the absolute path to the workspace.

  interaction_protocol:
    title: "Strict Adherence Required for ConPort Tool Calls"
    rules:
      - "Explain Intent: Before calling any ConPort tool, briefly explain to the USER *why* you are accessing the project memory (e.g., \"I'll check our project's decision log for previous choices on this topic.\")."
      - "No Tool Names: NEVER mention the specific ConPort tool names (e.g., `get_decisions`) in your conversation with the USER."
      - "Tool Call Placement: ALL ConPort MCP tool calls (e.g., `use_mcp_tool.../use_mcp_tool`) MUST be placed at the very END of your response message. Do not add any text after the tool call block."
      - "Wait for Results: If your next action depends on the output of a ConPort tool, ensure you have received and processed the result before proceeding. Your steps can be asynchronous, so explicitly wait if necessary by not requesting new tools until the ConPort result is available."

  proactive_memory_management:
    logging_guideline: |
      Actively identify opportunities to log new information (decisions, progress, patterns, glossary terms) into the project memory as it emerges in your conversation with the USER. Confirm with the USER before logging significant new entries or updates if you are inferring the information.
    updating_guideline: |
      Keep Product Context and Active Context up-to-date as project goals or current focus shift.

  tool_usage_focus:
    primary_use: |
      Use ConPort tools primarily for managing structured, persistent project knowledge.
    complementary_use: |
      Utilize your native tools (e.g., `Codebase Search`, `View File`) for direct code interaction and general information retrieval.
    mcp_server_type: |
      ConPort is a "tools-only" MCP server. Do not attempt to use MCP `prompts` or `resources` with it.

  initialization_guidance:
    # This guidance assumes the agent has already determined the `ACTUAL_WORKSPACE_ID`.
    # It also assumes the agent can check for the existence of the ConPort DB file
    # (e.g., `ACTUAL_WORKSPACE_ID + "/context_portal/context.db"`) using its native file system tools.

    on_existing_db_found: |
      # If an existing ConPort DB is found for the workspace:
      1. **Attempt to load existing project memory:** Call relevant ConPort `get_*` tools (e.g., `get_product_context`, `get_active_context`, `get_recent_activity_summary`).
      2. **Analyze loaded data:**
         - If data is successfully loaded and seems populated: Inform the USER "ConPort project memory loaded."
         - If data is minimal/empty despite DB existing: Inform USER "ConPort database found, but seems empty. You can start by defining Product Context."
         - Proceed with user's task, leveraging loaded context.

    on_no_db_found: |
      # If NO ConPort DB is found for the workspace:
      1. Inform USER: "No existing ConPort project memory found for this workspace."
      2. Ask USER: "Would you like to initialize ConPort for this workspace? A new data store will be created automatically when information is first saved." (Provide Yes/No options).
      3. **If USER agrees to initialize:**
         - Inform USER: "Okay, ConPort will be set up for this workspace."
         - **Check for `projectBrief.md` (ONLY on this initial setup):**
           - Use your file system tools to check if `projectBrief.md` exists in the workspace root.
           - If it exists:
             - Read its content.
             - Inform USER: "I found a `projectBrief.md` file."
             - Ask USER: "As we're setting up ConPort, would you like to import its content as the initial Product Context?" (Provide Yes/No options).
             - If USER agrees:
               - Prepare a content object for Product Context (e.g., `{"initial_brief": "[content of projectBrief.md]"}`).
               - Call ConPort's `update_product_context` (or equivalent operation to set Product Context) with this data.
               - Confirm import result with USER.
             - Else (`projectBrief.md` not found):
               - Ask USER: "`projectBrief.md` not found. Would you like to define the initial Product Context manually now?" (Provide Yes/No options).
               - (If Yes, guide user to provide input for `update_product_context`).
         - Inform USER: "ConPort is ready. You can now start logging decisions, progress, etc."
      4. **If USER declines to initialize:**
         - Inform USER: "Okay, ConPort will not be used for this session for this workspace."
         - (Agent should operate without relying on ConPort for this workspace).

    on_load_or_setup_failure: |
      # If any ConPort operations fail unexpectedly during the above, or if initial DB check itself fails:
      Inform USER: "There was an issue initializing or accessing ConPort project memory. Will proceed without it for now."
      (Agent should operate without relying on ConPort).

  configuration_reference:
    server_name_note: |
      The ConPort server (plugin) is typically named `conport-stdio` (or as configured by the user in `~/.codeium/windsurf/mcp_config.json`).
    detailed_strategy_note: |
      For detailed tool argument structures and advanced usage protocols, use the ConPort `get_conport_schema` tool for direct schema retrieval.

  user_driven_memory_sync:
    trigger_phrase_examples: # For Cascade's understanding of when to initiate this
      - "Sync ConPort"
      - "ConPort Sync"
      # - "Update project memory" # Keeping one old one for broader compatibility initially, can be removed later
      # - "Review and update our shared context"
    action_summary: |
      If the USER requests a "ConPort Sync" or similar:
      1. Acknowledge: Inform the USER with something like "[CONPORT_MEMORY: Syncing with chat session...]" or "Okay, I'll review our discussion and sync relevant information with ConPort."
      2. Review Chat: Analyze the current chat session for new information, decisions, progress, context changes, and clarifications that should be logged or updated in ConPort.
      3. Log/Update Systematically:
          - Explain to the USER what type of information you are about to save/update (e.g., "I'll log the decision we just made about the database schema.").
          - Use the appropriate ConPort tools (e.g., `log_decision`, `update_product_context`, `log_custom_data` for glossary terms) one by one, following all standard interaction protocols (explain intent, no tool names, tool call at end).
          - For `update_product_context` and `update_active_context`, it's often best to first retrieve the current content, then prepare the new full content or a patch, before calling the update tool.
      4. Summarize: After processing updates, briefly inform the USER that the project memory has been synchronized.
    example_flow_note: |
      Process is iterative; multiple tool calls may be needed per sync.

# --- Prompt Caching Strategies by Provider ---
# This file defines prompt caching strategies tailored for different LLM providers.
# An LLM assistant should select the relevant strategy based on its current model/provider.

prompt_caching_strategies:
  enabled: true # Master switch to enable/disable prompt caching strategies

  # --- Core Mandate (Applies to all enabled providers) ---
  core_mandate: |
    Actively seek opportunities to utilize prompt caching when interacting with the target LLM service.
    Primary goals: Reduce token costs and improve response latency.
    Leverage provider-specific caching mechanisms as defined below.
    - Notify user when structuring a prompt for potential caching: [INFO: Structuring prompt for caching]

  # --- Identifying Cacheable Content from ConPort (Applies to all providers) ---
  content_identification:
    description: |
      Criteria for identifying content from ConPort that is suitable for prompt caching.
      This content will form the stable prefix of prompts sent to the LLM.
    priorities:
      - item_type: "product_context"
        description: "Full text is a high-priority candidate if retrieved and relevant, due to size and relative stability."
      - item_type: "system_pattern"
        description: "Detailed descriptions of complex, frequently referenced patterns, especially if lengthy."
      - item_type: "custom_data"
        description: "Values from entries known/hinted to be large (e.g., specs, guides) or flagged with 'cache_hint: true' metadata."
      - item_type: "active_context"
        description: "Consider large, stable text blocks within active context if they will preface multiple queries *within the current task*."

    heuristics:
      min_token_threshold: 750 # Consider content for caching if it exceeds this approximate token count.
      stability_factor: "high" # Prefer content less likely to change rapidly within a short timeframe.

  # --- User-Defined Cache Hints (Applies to all providers) ---
  user_hints:
    description: |
      Users can provide explicit hints within ConPort item metadata to influence prompt caching decisions.
      These hints prioritize content for inclusion in the cacheable prompt prefix.

    retrieval: |
      When retrieving ConPort items that support metadata (e.g., `custom_data`), check the `metadata` field for the key `cache_hint`.
      If the `metadata` field is a JSON object and contains `"cache_hint": true`, consider the content of this item as a high-priority candidate for prompt caching, provided it also meets size and stability heuristics.

    logging_suggestion: |
      When logging or updating ConPort items (especially `custom_data`) that appear to be excellent caching candidates based on their size, stability, or likely reuse, you SHOULD suggest to the user adding a `cache_hint: true` flag to the item's `metadata` field.
      Confirm with the user before applying.
      Example suggestion: "This [Item Type, e.g., technical specification] seems large and stable, making it a good candidate for prompt caching. Would you like me to add `\"cache_hint\": true` to its metadata in ConPort to prioritize it?"

  # --- Strategy Note: Local Load vs. Provider Persistent Cache (Applies to all providers) ---
  strategy_note: |
    Storing cacheable content locally in ConPort and sending it as a prompt prefix at the start of each session avoids AI provider storage fees. However, this incurs the full input token cost for that content in every session and may increase initial latency compared to leveraging the provider's persistent caching with its discounted usage fees. The optimal approach depends on session frequency and content size. Provider-specific strategies below detail how to interact with their caching mechanisms.

  # --- Provider Specific Strategies ---

  gemini_api:
    description: Strategy for Google Gemini models (e.g., 1.5 Pro, 1.5 Flash) which support implicit caching.
    interaction_protocol:
      type: "implicit"
      details: |
        Leverage Gemini's implicit caching by structuring prompts.
        1. Retrieve the stable, cacheable context from ConPort (based on identification rules).
        2. Place this retrieved ConPort text at the *absolute beginning* of the prompt sent to Gemini.
        3. Append any variable, task-specific parts (e.g., user's specific question, code snippets for analysis) *after* the stable prefix.
        Example: "[Retrieved Product Context Text] \n\n Now, answer this specific question: [User's Question]"
    staleness_management:
      details: |
        Be aware that ConPort data can be updated. Cached versions of that data in Gemini have a TTL.
        While direct invalidation isn't typically managed via implicit caching APIs, structuring prompts consistently helps Gemini manage its cache.
        If you know a core piece of ConPort context (like Product Context) has just been updated, the *next* prompt you send using that context *as a prefix* will naturally cause Gemini to process and potentially re-cache the new version.

  anthropic_api:
    description: Strategy for Anthropic Claude models (e.g., 3.5 Sonnet, 3 Haiku, 3 Opus) which require explicit cache control.
    interaction_protocol:
      type: "explicit"
      details: |
        Utilize Anthropic's explicit prompt caching via `cache_control` breakpoints.
        1. Identify cacheable content from ConPort (based on identification rules and user hints).
        2. Construct the prompt message payload for the Anthropic API.
        3. Insert a `cache_control` breakpoint *after* the stable, cacheable content and *before* the variable content.
        Example (Conceptual API payload structure):
        {
          "messages": [
            {"role": "user", "content": "[Stable ConPort Content]"},
            {"role": "user", "content": {"type": "tool_code", "text": "<cache_control>{\"type\": \"set_cache_break\"}</cache_control>"}}, # Example breakpoint syntax
            {"role": "user", "content": "[Variable User Query]"}
          ],
          ...
        }
        (Note: The exact syntax for `cache_control` may vary; refer to Anthropic API docs.)
    staleness_management:
      details: |
        Anthropic's explicit caching may offer more control over invalidation or TTL, but details need confirmation from their API documentation.
        If ConPort data is updated, ensure subsequent prompts use the updated content, which should trigger re-caching or correct handling by the Anthropic API based on its specific rules.

  openai_api:
    description: Strategy for OpenAI models with automatic prompt caching.
    interaction_protocol:
      type: "implicit"
      details: |
        Leverage OpenAI's automatic prompt caching by structuring prompts.
        This is similar to Gemini's implicit caching and requires no explicit markers.
        1. Identify cacheable content from ConPort (based on identification rules and user hints).
        2. Place this retrieved ConPort text at the *absolute beginning* of the prompt sent to the OpenAI API.
        3. Append any variable, task-specific parts *after* the stable prefix.
        OpenAI provides a 50% discount on cached input tokens. Caching automatically activates for prompts over a certain length (e.g., >1024 tokens, but verify current documentation).
    staleness_management:
      details: |
        Automatic caching handles staleness implicitly. If prompt prefix changes (e.g., updated ConPort data), OpenAI processes/re-caches new prefix.

  other_providers:
    description: Placeholder for other LLM providers with prompt caching.
    interaction_protocol:
      type: "unknown" # Research needed
    staleness_management:
      details: "Research required."